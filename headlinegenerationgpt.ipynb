{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8216364,"sourceType":"datasetVersion","datasetId":4870049},{"sourceId":8216421,"sourceType":"datasetVersion","datasetId":4870099},{"sourceId":8252776,"sourceType":"datasetVersion","datasetId":4897055}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-29T11:55:49.456458Z","iopub.execute_input":"2024-04-29T11:55:49.457143Z","iopub.status.idle":"2024-04-29T11:55:49.842144Z","shell.execute_reply.started":"2024-04-29T11:55:49.457111Z","shell.execute_reply":"2024-04-29T11:55:49.841274Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/till-30000/till_30000.csv\n/kaggle/input/model-till-25000/config.json\n/kaggle/input/model-till-25000/merges.txt\n/kaggle/input/model-till-25000/vocab.json\n/kaggle/input/model-till-25000/tokenizer_config.json\n/kaggle/input/model-till-25000/model.safetensors\n/kaggle/input/model-till-25000/special_tokens_map.json\n/kaggle/input/model-till-25000/added_tokens.json\n/kaggle/input/model-till-25000/generation_config.json\n/kaggle/input/till-20000/till_20000.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2024-04-29T11:55:49.843953Z","iopub.execute_input":"2024-04-29T11:55:49.844590Z","iopub.status.idle":"2024-04-29T11:55:52.824425Z","shell.execute_reply.started":"2024-04-29T11:55:49.844554Z","shell.execute_reply":"2024-04-29T11:55:52.823604Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/till-30000/till_30000.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T11:55:52.826148Z","iopub.execute_input":"2024-04-29T11:55:52.826713Z","iopub.status.idle":"2024-04-29T11:55:54.459335Z","shell.execute_reply.started":"2024-04-29T11:55:52.826680Z","shell.execute_reply":"2024-04-29T11:55:54.458286Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df1 = pd.read_csv(\"/kaggle/input/till-20000/till_20000.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T11:56:00.353146Z","iopub.execute_input":"2024-04-29T11:56:00.353780Z","iopub.status.idle":"2024-04-29T11:56:00.731666Z","shell.execute_reply.started":"2024-04-29T11:56:00.353749Z","shell.execute_reply":"2024-04-29T11:56:00.730643Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"df.drop(\"Unnamed: 0\", axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T11:56:00.734476Z","iopub.execute_input":"2024-04-29T11:56:00.735113Z","iopub.status.idle":"2024-04-29T11:56:00.749189Z","shell.execute_reply.started":"2024-04-29T11:56:00.735078Z","shell.execute_reply":"2024-04-29T11:56:00.748179Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-29T11:56:01.135212Z","iopub.execute_input":"2024-04-29T11:56:01.136096Z","iopub.status.idle":"2024-04-29T11:56:01.151215Z","shell.execute_reply.started":"2024-04-29T11:56:01.136052Z","shell.execute_reply":"2024-04-29T11:56:01.150309Z"},"trusted":true},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                               title  \\\n0  You Need to Care About Facebook Censoring an I...   \n1  Explainer: Counting the cost of the U.S.-China...   \n2  'Breaking Bad' Star Lavell Crawford Dropped 12...   \n3                 Daily Horoscope: November 09, 2016   \n4  UK's May tries to calm Brexit rebels, says dea...   \n\n                                             article  \\\n0  Nick Ut's Pulitzer Prize-winning photograph of...   \n1  HONG KONG (Reuters) - U.S. President Donald Tr...   \n2  Comedian and actor Lavell Crawford's not quite...   \n3  Mars enters Aquarius today at 12:52 AM! Mars i...   \n4  LONDON (Reuters) - Prime Minister Theresa May ...   \n\n                                             summary  \n0  Nick Ut's Pulitzer Prize-winning photograph of...  \n1  U.S. President Donald Trump's fresh threats of...  \n2  Comedian and actor Lavell Crawford's not quite...  \n3  Mars enters Aquarius today at 12:52 AM! Mars i...  \n4  LONDON (Reuters) - Prime Minister Theresa May ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>article</th>\n      <th>summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>You Need to Care About Facebook Censoring an I...</td>\n      <td>Nick Ut's Pulitzer Prize-winning photograph of...</td>\n      <td>Nick Ut's Pulitzer Prize-winning photograph of...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Explainer: Counting the cost of the U.S.-China...</td>\n      <td>HONG KONG (Reuters) - U.S. President Donald Tr...</td>\n      <td>U.S. President Donald Trump's fresh threats of...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>'Breaking Bad' Star Lavell Crawford Dropped 12...</td>\n      <td>Comedian and actor Lavell Crawford's not quite...</td>\n      <td>Comedian and actor Lavell Crawford's not quite...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Daily Horoscope: November 09, 2016</td>\n      <td>Mars enters Aquarius today at 12:52 AM! Mars i...</td>\n      <td>Mars enters Aquarius today at 12:52 AM! Mars i...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>UK's May tries to calm Brexit rebels, says dea...</td>\n      <td>LONDON (Reuters) - Prime Minister Theresa May ...</td>\n      <td>LONDON (Reuters) - Prime Minister Theresa May ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"df['summary'] = df['summary'].str.replace(r'\\s{2,}', ' ', regex=True).str.strip()","metadata":{"execution":{"iopub.status.busy":"2024-04-29T11:56:02.739727Z","iopub.execute_input":"2024-04-29T11:56:02.740080Z","iopub.status.idle":"2024-04-29T11:56:03.413518Z","shell.execute_reply.started":"2024-04-29T11:56:02.740051Z","shell.execute_reply":"2024-04-29T11:56:03.412706Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"'''from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\ntokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\nmodel.resize_token_embeddings(len(tokenizer))'''\n\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\nmodel_path = \"/kaggle/input/model-till-25000\"\n\n# # Load the model\n# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n# tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\ntokenizer = GPT2Tokenizer.from_pretrained(model_path)\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\n# model.resize_token_embeddings(len(tokenizer))\n\n# If GPU is available, move the model to GPU\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T12:03:11.149808Z","iopub.execute_input":"2024-04-29T12:03:11.150169Z","iopub.status.idle":"2024-04-29T12:03:24.994653Z","shell.execute_reply.started":"2024-04-29T12:03:11.150140Z","shell.execute_reply":"2024-04-29T12:03:24.993666Z"},"trusted":true},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50258, 1024)\n    (wpe): Embedding(1024, 1024)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-23): 24 x GPT2Block(\n        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1024, out_features=50258, bias=False)\n)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\ntorch.random.manual_seed(0)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/Phi-3-mini-128k-instruct\", \n    device_map=\"cuda\", \n    torch_dtype=\"auto\", \n    trust_remote_code=True, \n)\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")\n\nmessages = df[\"summary\"][1]\n\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n)\n\ngeneration_args = {\n    \"max_new_tokens\": 500,\n    \"return_full_text\": False,\n    \"temperature\": 0.0,\n    \"do_sample\": False,\n}\n\noutput = pipe(messages, **generation_args)\nprint(output[0]['generated_text'])\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T12:01:35.891657Z","iopub.execute_input":"2024-04-29T12:01:35.892367Z","iopub.status.idle":"2024-04-29T12:02:08.764237Z","shell.execute_reply.started":"2024-04-29T12:01:35.892333Z","shell.execute_reply":"2024-04-29T12:02:08.763301Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a32a49709dd649e38b41b9ef89b943f1"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":" But the trade war has also hurt China's economy. The country's economic growth has slowed to 6.5 percent in the first quarter of this year, the lowest in 26 years.The trade war has also hurt the U.S. economy. The International Monetary Fund has cut its forecast for U.S. growth this year to 2.1 percent from 2.3 percent. The U.S. economy grew at a 2.9 percent annual rate in the first quarter of this year.The U.S. trade deficit with China has also widened. The deficit was $37.2 billion in March, the highest since September 2008.The U.S. trade deficit with China has widened to $37.2 billion in March, the highest since September 2008. The deficit was $37.1 billion in February and $36.6 billion in January.The U.S. trade deficit with China has widened to $37.2 billion in March, the highest since September 2008. The deficit was $37.1 billion in February and $36.6 billion in January.The U.S. trade deficit with China has widened to $37.2 billion in March, the highest since September 2008. The deficit was $37.1 billion in February and $36.6 billion in January.The U.S. trade deficit with China has widened to $37.2 billion in March, the highest since September 2008. The deficit was $37.1 billion in February and $36.6 billion in January.The U.S. trade deficit with China has widened to $37.2 billion in March, the highest since September 2008. The deficit was $37.1 billion in February and $36.6 billion in January.The U.S. trade deficit with China has widened to $37.2 billion in March, the highest since September 2008. The deficit was $37.1 billion in February and $36.6 billion in January\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"### **Training GPT2**","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nfrom datasets import Dataset, load_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"gpt2_summary\"] = df[\"summary\"] + '\\nTL;DR:' + df[\"title\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1[\"gpt2_summary\"] = df1[\"summary\"] + '\\nTL;DR:' + df1[\"title\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_df = df[['gpt2_summary']].iloc[2000:4000]\neval_df = df1[['gpt2_summary']].iloc[4000:4100]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_df = Dataset.from_pandas(new_df)\neval_df = Dataset.from_pandas(eval_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_examples_to_features(example_batch):\n    input_encodings = tokenizer(example_batch['summary'] , max_length = 1024, padding = True, truncation = True )\n    \n    with tokenizer.as_target_tokenizer():\n        target_encodings = tokenizer(example_batch['title'], max_length = 32, padding = True, truncation = True )\n        \n    return {\n        'input_ids' : input_encodings['input_ids'],\n        'attention_mask': input_encodings['attention_mask'],\n        'labels': target_encodings['input_ids']\n    }\n    \nnew_df_pt = new_df.map(convert_examples_to_features, batched = True)\neval_df_pt = eval_df.map(convert_examples_to_features, batched = True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\nseq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\nfrom accelerate import Accelerator, DataLoaderConfiguration\n\n# Create DataLoaderConfiguration object\ndataloader_config = DataLoaderConfiguration(\n    dispatch_batches=None,  # Change None to your desired value\n    split_batches=False,\n    even_batches=True,\n    use_seedable_sampler=True\n)\n\n# Create Accelerator object with DataLoaderConfiguration\naccelerator = Accelerator(dataloader_config=dataloader_config)\n\n\n# Define training arguments with gradient accumulation, dropout, and validation\ntrainer_args = TrainingArguments(\n    output_dir='news_headline',\n    num_train_epochs=10,\n    warmup_steps=500,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    weight_decay=0.01,\n    logging_steps=15,\n    gradient_accumulation_steps=16,\n    learning_rate=3e-5,  # Specify the learning rate here\n    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n    overwrite_output_dir=True,\n    dataloader_num_workers=accelerator.num_processes\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Now you can use the accelerator object in your Trainer initialization\ntrainer = Trainer(\n    model=model,\n    args=trainer_args,\n    tokenizer=tokenizer,\n    data_collator=seq2seq_data_collator,\n    train_dataset=new_df_pt,\n    eval_dataset=eval_df_pt\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_result = trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.evaluate(metric_key_prefix='test_en',\n                eval_dataset=en_test_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model =  model.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sentence = output[0]['generated_text']\nquery = sentence + '\\nTL;DR:'","metadata":{"execution":{"iopub.status.busy":"2024-04-29T12:04:30.775530Z","iopub.execute_input":"2024-04-29T12:04:30.775898Z","iopub.status.idle":"2024-04-29T12:04:30.780421Z","shell.execute_reply.started":"2024-04-29T12:04:30.775858Z","shell.execute_reply":"2024-04-29T12:04:30.779382Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"input_ids = tokenizer(query, return_tensors='pt')","metadata":{"execution":{"iopub.status.busy":"2024-04-29T12:04:44.047546Z","iopub.execute_input":"2024-04-29T12:04:44.048187Z","iopub.status.idle":"2024-04-29T12:04:44.059562Z","shell.execute_reply.started":"2024-04-29T12:04:44.048153Z","shell.execute_reply":"2024-04-29T12:04:44.058773Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Move the input to GPU if available\ninput_ids = input_ids.to(\"cuda\")\n\n# Generate 5 different outputs\nnum_outputs = 7\ngenerated_outputs = []\nmax_new_tokens = 20\nfor _ in range(num_outputs):\n    # Generate output using the model\n    output = model.generate(\n        input_ids=input_ids[\"input_ids\"],\n        attention_mask=input_ids['attention_mask'],\n        length_penalty=0.8,\n        min_new_tokens=7,\n        max_new_tokens=max_new_tokens+_,\n        num_beams=8,\n        no_repeat_ngram_size=2,\n        do_sample=True,\n        early_stopping=True\n    )\n    generated_outputs.append(tokenizer.decode(output[0],skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-04-29T12:04:50.787578Z","iopub.execute_input":"2024-04-29T12:04:50.788277Z","iopub.status.idle":"2024-04-29T12:04:58.846211Z","shell.execute_reply.started":"2024-04-29T12:04:50.788235Z","shell.execute_reply":"2024-04-29T12:04:58.845442Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Print the generated outputs\nfor i, output_text in enumerate(generated_outputs):\n    # Find the index of \"TL;DR:\"\n    tldr_index = output_text.find(\"TL;DR:\")\n    if tldr_index != -1:\n        # Extract the text after \"TL;DR:\"\n        output_text = output_text[tldr_index + len(\"TL;DR:\"):]\n    \n    # Remove commas from the output text\n    output_text = output_text.replace(\",\", \"\")\n\n    print(f\"Output {i+1}: {output_text.strip()}\")  # Strip to remove leading/trailing whitespace","metadata":{"execution":{"iopub.status.busy":"2024-04-29T12:04:58.847782Z","iopub.execute_input":"2024-04-29T12:04:58.848134Z","iopub.status.idle":"2024-04-29T12:04:58.854424Z","shell.execute_reply.started":"2024-04-29T12:04:58.848101Z","shell.execute_reply":"2024-04-29T12:04:58.853558Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Output 1: China's trade with the United States is hurting its economy but the damage is far from done\nOutput 2: Factbox: China trade with the United States is hurting the world economy and it's getting worse\nOutput 3: China's trade with the United States is hurting the economy  : This article originally appeared on Rec\nOutput 4: China trade woes hit China economy but it's not as bad as it used to Be sure to sign up for\nOutput 5: China's trade with the United States is hurting the world economy but the damage is far worse than you think\nOutput 6: China's trade with the United States is hurting its economy but it's not as bad as it used to be\nOutput 7: Factbox: China trade with the United States is the biggest drag on the world economy  - The Wall Street Journal\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"df['title'][1] ","metadata":{"execution":{"iopub.status.busy":"2024-04-29T12:05:02.063142Z","iopub.execute_input":"2024-04-29T12:05:02.063821Z","iopub.status.idle":"2024-04-29T12:05:02.069870Z","shell.execute_reply.started":"2024-04-29T12:05:02.063786Z","shell.execute_reply":"2024-04-29T12:05:02.068864Z"},"trusted":true},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'Explainer: Counting the cost of the U.S.-China trade war so far'"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"!pip install rouge","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sentence = '''\nBENGALURU: Over 69% of voters in 14 Lok Sabha constituencies of coastal and southern Karnataka turned up to exercise their franchise on Friday, and the provisional voting percentage almost mirrors the figures notched up in the previous two polls: 70.4% in 2019 and 68% in 2014.\nBengaluru continued to remain on a sticky wicket with its overall subpar performance of 57%.Mysuru, for the first time, hit the ball out of the park crossing 70% against 69.5% in 2019 and 67.3% in 2014. With this round of polling, the fate of 247 candidates has been sealed, and the remaining 14 seats in central and northern Karnataka will vote on May 7.\nThe seemingly middling turnout in the first phase is yet commendable as voters braved the heat to keep their tryst with the voting exercise. After the final tabulation on the morrow, the overall voting percentage is expected to rise by a couple of percentage of points, helping the figure sail over the psychological 70 mark.\nOf the four seats, Bangalore Rural reported the best turnout of 67.3% compared to 64.9% in 2019 and Central recor ded 52.8% compared to 54.3% in 2019, while it was 53.1% in South (53.7% in 2019). North had breached the 50% mark by 5pm and ended with 54.4%, compared to 54.8% in 2019.\nAll four seats had seen a dip in turnout compared to 2014, with Bangalore South recording the least.\nThe low turnout in the four-seat Bengaluru cluster is attributable to high temperatures and largely to continuing apathy. The issue of electoral roll inflation also continues to plague Bengaluru.\nMandya, which had seen a lot of campaigning action in the run-up to the polling day, emerged on top again, registering 81.5% turnout.\nIn 2019 and 2014 too, the tally had crossed 80% mark, making them the most robust voting constituency in the state.\nThe 14 seats continued to witness the trend of rural voters pulling up the overall voting percentage in Karnataka with Bengaluru’s low proving to be a drag. The lowest polling was recorded in Bangalore Central with just over 52%.\nIn a bid to beat the late forenoon and afternoon heat, voters stood in long queues early in the morning across the state.\nThe voter turnout was the highest between 7am and 11am with 22% and post 3pm when Karnataka saw close to 18% of the total voters casting their ballot.\nBarring isolated cases of vandalism, missing votes and mismatches in the electoral roll, leading to a few voters being unable to cast their ballot, the state saw peaceful voting across the constituencies.\nAs the day progressed and temperatures soared past 37°C, Bengalureans failed to show much enthusiasm in exercising their franchise.\n“We expected the city to perform better this time. After multiple consultations and reviews with local authorities, we had identified problematic areas and planned a targeted approach to educate voters and create awareness. Yet, the city did n ot improve on its previous tally,” said a senior official from the Chief Elec toral Office of Karnataka.\nPolitical parties, too, were upset about the outcome and blamed stringent measures initiated by the Election Commission for the poor turnout.\nA candidate from Bangalore Central said: “There are restrictions on holding community engagement events in clubs and marriage halls. We will be charged expenses if a bike or any other rally is held. Given the time constraint, it is difficult to connect with a ll voters through doorto-door campaigns alone.”\n“More events would have increased awareness among the general public. Digital platforms help in reaching out to one section of society but not all,” the candidate pointed out.'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokens = tokenizer.tokenize(sentence)\n\n# Count the number of tokens\nnum_tokens = len(tokens)\n\nprint(\"Number of tokens:\", num_tokens)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom rouge import Rouge\n\n\n# Define the generated summary and the reference summary\nreference_summary = df[\"title\"][1900]\n\n# Initialize the ROUGE object\nrouge = Rouge()\n\n# Define the input text for GPT-2\ninput_text = summarize(model_bert, df[\"article\"][1900]) + '\\nTL;DR:'\n\n# Tokenize input text\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(\"cuda\")\n\n# Generate predictions\noutput = model.generate(input_ids=input_ids,\n                        length_penalty=0.8,\n                        min_new_tokens=7,\n                        max_new_tokens=24,\n                        num_beams=8,\n                        no_repeat_ngram_size=2,\n                        do_sample=True,\n                        early_stopping=True,\n                        temperature=0.7,\n                        top_k=50,\n                        top_p=0.95,\n                        num_return_sequences=2)\n\n# Decode the generated sequence\noutput_text = tokenizer.decode(output[0], skip_special_tokens=True)\ntldr_index = output_text.find(\"TL;DR:\")\nif tldr_index != -1:\n    # Extract the text after \"TL;DR:\"\n    output_text = output_text[tldr_index + len(\"TL;DR:\"):]\n\n#Remove commas from the output text\noutput_text = output_text.replace(\",\", \"\")\n\ngenerated_text = output_text.strip()\n\n# Calculate ROUGE for the generated and reference summaries\nscores = rouge.get_scores(generated_text, reference_summary)\n# Print the results\nprint(scores)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings \nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\n\ndef calculate_rouge_scores(df, model, tokenizer):\n    rouge_scores = []\n\n    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating ROUGE scores\"):\n        # Define the reference summary\n        reference_summary = row[\"title\"]\n        max_new_tokens = len(reference_summary)\n\n        # Define the input text for GPT-2\n        input_text = summarize(model_bert, row[\"article\"]) + '\\nTL;DR:'\n\n        # Tokenize input text\n        input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\n        # Generate predictions\n        output = model.generate(input_ids=input_ids[\"input_ids\"],\n                                attention_mask=input_ids[\"attention_mask\"],\n                                length_penalty=0.8,\n                                min_new_tokens=7,\n                                max_new_tokens=max_new_tokens,\n                                num_beams=8,\n                                no_repeat_ngram_size=2,\n                                do_sample=True,\n                                early_stopping=True,\n                                temperature=0.7,\n                                top_k=50,\n                                top_p=0.95,\n                                num_return_sequences=2)\n\n        # Decode the generated sequence\n        output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n        tldr_index = output_text.find(\"TL;DR:\")\n        if tldr_index != -1:\n            # Extract the text after \"TL;DR:\"\n            output_text = output_text[tldr_index + len(\"TL;DR:\"):]\n\n        # Remove commas from the output text\n        output_text = output_text.replace(\",\", \"\")\n\n        generated_text = output_text.strip()\n\n        # Calculate ROUGE for the generated and reference summaries\n        scores = rouge.get_scores(generated_text, reference_summary)\n        rouge_scores.append(scores)\n\n    return rouge_scores\n\n# Example usage:\nrouge_scores = calculate_rouge_scores(df[:400], model, tokenizer)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute average scores\nrouge1_avg = sum(score[0]['rouge-1']['f'] for score in rouge_scores) / len(rouge_scores)\nrouge2_avg = sum(score[0]['rouge-2']['f'] for score in rouge_scores) / len(rouge_scores)\nrougeL_avg = sum(score[0]['rouge-l']['f'] for score in rouge_scores) / len(rouge_scores)\n\n# Create a dictionary of average scores\navg_scores = {\n    'rouge1_avg': rouge1_avg*100,\n    'rouge2_avg': rouge2_avg*100,\n    'rougeL_avg': rougeL_avg*100\n}\n\n# Display average scores in a table\ndf_avg_scores = pd.DataFrame.from_dict(avg_scores, orient='index', columns=['Average Score'])\nprint(df_avg_scores)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generated_text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"gpt2_summary\"][13]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install bert_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from bert_score import score\n\nhypotheses = [df[\"title\"][1900]]\n# Define the input text for GPT-2\ninput_text = df[\"summary\"][29] + '\\nTL;DR:'\n\n# Tokenize input text\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(\"cuda\")\n\n# Generate predictions\noutput = model.generate(input_ids=input_ids,\n                        length_penalty=0.8,\n                        min_new_tokens=7,\n                        max_new_tokens=30,\n                        num_beams=8,\n                        no_repeat_ngram_size=2,\n                        do_sample=True,\n                        early_stopping=True,\n                        temperature=0.7,\n                        top_k=50,\n                        top_p=0.95,\n                        num_return_sequences=1)\n\n# Decode the generated sequence\noutput_text = tokenizer.decode(output[0], skip_special_tokens=True)\ntldr_index = output_text.find(\"TL;DR:\")\nif tldr_index != -1:\n    # Extract the text after \"TL;DR:\"\n    output_text = output_text[tldr_index + len(\"TL;DR:\"):]\n\n# Remove commas from the output text\noutput_text = output_text.replace(\",\", \"\")\n\ngenerated_text = output_text.strip()\nreferences = [generated_text]\n\nlang = \"en\"\n\nP, R, F1 = score(hypotheses, references, lang=lang)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Precision: \", P[0],\"\\nCoverage: \",R[0], \"\\nF1 Score: \",F1[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U nltk","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\nfrom nltk.translate.meteor_score import meteor_score\n\nreference = [df[\"title\"][2900].split()]\ninput_text = df[\"summary\"][2900] + '\\nTL;DR:'\nmax_new_tokens = len(reference[0])\n# Tokenize input text\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\n# Generate predictions\noutput = model.generate(input_ids=input_ids[\"input_ids\"],\n                        attention_mask=input_ids['attention_mask'],\n                        length_penalty=0.8,\n                        min_new_tokens=7,\n                        max_new_tokens=30,\n                        num_beams=8,\n                        no_repeat_ngram_size=2,\n                        do_sample=True,\n                        early_stopping=True)\n\n# Decode the generated sequence\noutput_text = tokenizer.decode(output[0], skip_special_tokens=True)\ntldr_index = output_text.find(\"TL;DR:\")\nif tldr_index != -1:\n    # Extract the text after \"TL;DR:\"\n    output_text = output_text[tldr_index + len(\"TL;DR:\"):]\n\n# Remove commas from the output text\noutput_text = output_text.replace(\",\", \"\")\n\ngenerated_text = output_text.strip()\ngenerated = generated_text.split()\nscore = meteor_score(reference, generated)\nprint(score)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generated_text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torchtext.data.metrics import bleu_score\n\n# define the source text and reference text\nsource_text = [\"This is an example message for summarization.\"]\nreference_text = [\"This is an example message for summarizing.\"]\n# define the text generated by the model\ngenerated_text = [\"This is an example message for summarizing.\"]\n# calculate the BLEU score\nscore = bleu_score(generated_text, reference_text)\nprint(f'BLEU Score: {score*100:.2f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sentence = summarization('''Think of Ian Nepomniachtchi not as a grandmaster, but as an escape artist. Time and again at the Candidates tournament in Toronto, he’s found himself in losing positions, like he did against Praggnanandhaa in Round 5 and against Alireza Firouzja in Round 9. Time and again, he’s escaped unscathed. On Thursday, Nepomniachtchi performed his most jaw-dropping trick in Toronto yet, managing to survive from two losing positions before defeating Vidit Gujrathi after a marathon battle that lasted over five hours.\nThe victory sent Nepomniachtchi into sole lead in the open section of the Candidates chess tournament with seven points. Nepomniachtchi has Gukesh, the 17-year-old, first-timer from India, and 36-year-old Hikaru Nakamura, a two-time Candidates contender, hot on his heels.\nBoth have half a point less than Nepomniachtchi with just three more rounds left. Gukesh played out a draw against World No.2 Fabiano Caruana with white pieces while the third Indian in the open competition, Praggnanandhaa, lost to World No.3 Nakamura, a result that sees him 1.5 points off the pace.\nChess CandidatesWhen the end came, five hours and 67 moves later, both players were in shock. (Michal Walusza via FIDE)\nThe scenario could have been a lot different had the chips fallen differently in the Round 11 game against Vidit. Had the Indian managed to defeat Nepo, Gukesh and Nakamura would have been joint leaders. Had he managed to draw with the Russian, there would have been three joint leaders heading into Round 12.\nWhen the end came, five hours and 67 moves later, both players were in shock. Vidit shook his head, held his face in both palms as he walked away. It was dawning on him that defeat left him two points behind the Russian, meaning his hopes of winning the Candidates are virtually over.\nNepo sat on his chair for a while, shuttered his eyes with his palm, scarcely believing that he had managed to claw a victory out of a game he seemed destined to lose. Twice.\nHow Vidit allowed Nepo to escape twice\nBy the 33rd move, Vidit, playing with white pieces, had built up a significant advantage on the board.\nChess Candidates\nHe could have added to that advantage by advancing his pawn in the ‘h file’ by one more square to h5, which would have put the black pawn on ‘g file’ under pressure: it could either capture the advancing pawn, or be captured by it. Either way, had Vidit made that move, there would be a pawn advancing towards Nepo’s king, not to mention the rook on the first rank who could have darted sideways before charging at Nepo’s king on h8.\nInstead, Vidit chose the move 34. Kd3, which the computer didn’t appreciate too much.\n“I was doing fine after that (move). But I wanted to win the game,” Nepo said before launching into an analysis of what he had thought during the game and its critical moments, a monologue he ended with: “I got lucky.”\nHe would get lucky again. The second reprieve Vidit inadvertently offered Nepomniachtchi was three moves later when instead of claiming the pawn on d5 with his knight, Vidit chose to withdraw his king to c2.\nIt was a decision that Vidit made in a hurry, because his time on the clock was thinning. Had Vidit chosen to claim the pawn with his knight, it would have started off a massacre on the board with two knights, two pawns, one bishop and two rooks losing their lives but Vidit emerging with a significant advantage.\nLater, Nepo admitted he had not calculated what would happen if Vidit claimed the pawn with his knight. “Oh, white would win? Good to know,” he said with a straight face at the post-match press conference before the shadow of a smile appeared on his face as he added earnestly. “I didn’t calculate that of course.”\nThere was one more moment where Vidit could have chosen to repeat his move and push the game to a draw. But he chose differently.\nOn the 41st move, Vidit pushed his knight to d2, which got him a bishop d6 response. Vidit promptly shifted his knight to e4, attacking the black bishop. Nepo shifted it to the initial square again, which meant that the position had repeated twice on the board. All Vidit had to do was move his knight to d2 again. But he chose Ng3, which indicated he wasn’t looking for a draw.\n“Luckily for me, he didn’t take the draw here and went Ng3,” Nepo said.\nNepomniachtchi has an enviable record at the Candidates. He won the 2020 edition, which was split in half by the coronavirus pandemic — the first half being played in March 2020 and the second half starting 13 months later in April 2021 in Yekaterinburg. But Nepo stayed consistent through that phase. He was joint leader when the event was stopped, and confirmed his victory with a round to spare when the players reconvened in 2021. In 2022, the event was clouded with a different kind of uncertainty: whether Magnus Carlsen will defend his title against the Candidates winner or not. Even at that edition, Nepo managed to win with a round to spare.\nChess CandidatesThe man who has a 100 percent success rate at the Candidates tournament is now one square ahead of the rest of the eight-player field. Gukesh and Nakamura have just three more moves left to catch the man who’s an escape artist on the 64 squares. (Michal Walusza via FIDE)\nThe man who has a 100 percent success rate at the Candidates tournament is now one square ahead of the rest of the eight-player field. Gukesh and Nakamura have just three more moves left to catch the man who’s an escape artist on the 64 squares.''')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query = sentence + '\\nTL;DR:'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\nfrom datasets import Dataset, load_dataset\n\ndevice =  \"cuda\"\n\nmodel_ckpt = \"facebook/bart-large-cnn\"\n\ntokenizer1 = AutoTokenizer.from_pretrained(model_ckpt)\n\nmodel_summary = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device) \npipe = pipeline('summarization', model=model_summary, tokenizer = tokenizer1,device = device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import pipeline\n\n# Example function to generate segments of text within the maximum sequence length\ndef generate_segments(input_text):\n    words = input_text.split()\n    for i in range(0, len(words), 300):  # Adjust the segment length here\n        yield ' '.join(words[i:i+300])   # Adjust the segment length here\n\ndef summarization(input_text):\n    if(len(tokenizer.tokenize(input_text))<=1024):\n        return input_text\n    \n    global count\n    output_summary = \"\"\n    for segment in generate_segments(input_text):\n        try:\n            max_length = len(segment.split()) # Adjust max_length dynamically\n            summarized_segment = pipe(segment, max_length=max_length, min_length=1, do_sample=False)[0]['summary_text']\n            output_summary += summarized_segment\n            \n            if len(output_summary.split()) >= 1024:\n                break\n        except IndexError:\n            print(\"Index error occurred, skipping this segment.\")\n        except Exception as e:\n            print(f\"Error occurred: {e}\")\n    count += 1\n    print(count, end=\" \")\n    return output_summary\ncount = 0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"headline_GPT2\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\n\nfile_path = \"/kaggle/working/gpt2-fp16.tflite\"  # Specify the full path to the file\nlink = FileLink(file_path)\nlink\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T12:20:51.360850Z","iopub.execute_input":"2024-04-28T12:20:51.361285Z","iopub.status.idle":"2024-04-28T12:20:51.368985Z","shell.execute_reply.started":"2024-04-28T12:20:51.361255Z","shell.execute_reply":"2024-04-28T12:20:51.367978Z"},"trusted":true},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/gpt2-fp16.tflite","text/html":"<a href='/kaggle/working/gpt2-fp16.tflite' target='_blank'>/kaggle/working/gpt2-fp16.tflite</a><br>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"Ateeqq/news-title-generator\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Ateeqq/news-title-generator\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_title(input_text):\n\n    # Encode the input text\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n\n    # Generate the output\n    output = model.generate(input_ids)\n\n    # Decode the generated tokens\n    decoded_text = tokenizer.decode(output[0], skip_special_tokens=True)\n    return decoded_text\n\n# Example usage\ninput_text = df[\"article\"][1900]\ngenerated_title = generate_title(input_text)\n\n# print(f\"Input Text: {input_text}\")\nprint(f\"Generated Title: {generated_title}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"title\"][1900]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom rouge import Rouge\n\n\n# Define the generated summary and the reference summary\nreference_summary = df[\"title\"][1900]\n\n# Initialize the ROUGE object\nrouge = Rouge()\n\n\n# Calculate ROUGE for the generated and reference summaries\nscores = rouge.get_scores(generated_title, reference_summary)\n# Print the results\nprint(scores)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install bert-extractive-summarizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import transformers\nfrom summarizer import Summarizer, TransformerSummarizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_bert = TransformerSummarizer( transformer_model_key=\"mrm8488/bert-mini2bert-mini-finetuned-cnn_daily_mail-summarization\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_bert = Summarizer()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#helper  function\ndef summarize(model, text):\n    summary = ''.join(model(text))\n    return(summary)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summarize(model_bert, df[\"article\"][1900])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"article\"][1900]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import TFGPT2LMHeadModel\n\nmodel = TFGPT2LMHeadModel.from_pretrained('/kaggle/input/model-till-25000')\n\ninput_spec = tf.TensorSpec([1, 64], tf.int32)\nmodel._set_inputs(input_spec, training=False)\n\nprint(model.inputs)\nprint(model.outputs)\n\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\n\n# FP16 quantization:\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_types = [tf.float16]\n\ntflite_model = converter.convert()\n\nopen(\"gpt2-fp16.tflite\", \"wb\").write(tflite_model)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T12:07:46.462181Z","iopub.execute_input":"2024-04-28T12:07:46.462566Z","iopub.status.idle":"2024-04-28T12:11:37.994716Z","shell.execute_reply.started":"2024-04-28T12:07:46.462534Z","shell.execute_reply":"2024-04-28T12:11:37.993517Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2024-04-28 12:07:48.234891: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-28 12:07:48.235018: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-28 12:07:48.369552: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nAll PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n\nAll the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"None\nNone\n","output_type":"stream"},{"name":"stderr","text":"Summary on the non-converted ops:\n---------------------------------\n * Accepted dialects: tfl, builtin, func\n * Non-Converted Ops: 319, Total Ops 3153, % non-converted = 10.12 %\n * 319 ARITH ops\n\n- arith.constant:  319 occurrences  (f16: 299, i32: 20)\n\n\n\n  (f32: 291, i32: 24)\n  (f32: 49)\n  (f32: 25)\n  (f32: 299)\n  (i32: 1)\n  (f32: 96)\n  (f32: 2)\n  (f32: 24)\n  (i1: 24)\n  (f32: 98)\n  (f32: 220)\n\n  (f32: 24, i32: 220)\n  (i32: 49)\n  (f32: 313, i32: 27)\n  (f32: 49)\n  (i32: 219)\n  (f32: 24)\n  (f32: 24)\n  (f32: 49)\n  (i32: 461)\n  (f32: 98, i32: 24)\n  (f32: 96)\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"710222180"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}