{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8036227,"sourceType":"datasetVersion","datasetId":4737418},{"sourceId":8252776,"sourceType":"datasetVersion","datasetId":4897055}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-29T11:55:08.479413Z","iopub.execute_input":"2024-04-29T11:55:08.480240Z","iopub.status.idle":"2024-04-29T11:55:09.362211Z","shell.execute_reply.started":"2024-04-29T11:55:08.480206Z","shell.execute_reply":"2024-04-29T11:55:09.361251Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/news-2-0/news_2.0.csv\n/kaggle/input/model-till-25000/config.json\n/kaggle/input/model-till-25000/merges.txt\n/kaggle/input/model-till-25000/vocab.json\n/kaggle/input/model-till-25000/tokenizer_config.json\n/kaggle/input/model-till-25000/model.safetensors\n/kaggle/input/model-till-25000/special_tokens_map.json\n/kaggle/input/model-till-25000/added_tokens.json\n/kaggle/input/model-till-25000/generation_config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import pipeline\n\nimport matplotlib.pyplot as plt\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\nimport torch.optim as optim\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-04-29T11:55:09.364176Z","iopub.execute_input":"2024-04-29T11:55:09.364683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = pd.read_csv(\"/kaggle/input/news-2-0/news_2.0.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nmodel_ckpt = \"facebook/bart-large-cnn\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# pipe = pipeline('summarization', model=model_ckpt, device = device)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T08:26:18.821169Z","iopub.execute_input":"2024-04-29T08:26:18.821453Z","iopub.status.idle":"2024-04-29T08:26:30.817612Z","shell.execute_reply.started":"2024-04-29T08:26:18.821429Z","shell.execute_reply":"2024-04-29T08:26:30.816580Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"888b9d64e3c64cf2a46f0e0dddc74fd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d43264cdb8a64f7ba83cfe8f9c3f8ef7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5637201a0fe41e58b49ada4434ca9d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e35d6a6a42864f43a80438cc893f8185"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e81750787ea3435987a19faf3f6caf3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc2c402140e249bf8382759b13fcfa95"}},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom transformers import pipeline\n\n# Example function to generate segments of text within the maximum sequence length\ndef generate_segments(input_text):\n    words = input_text.split()\n    for i in range(0, len(words), 300):  # Adjust the segment length here\n        yield ' '.join(words[i:i+300])   # Adjust the segment length here\n\ndef summarization(input_text):\n    global count\n    if(len(tokenizer.tokenize(input_text))<=1020):\n        count += 1\n        print(count, end=\" \")\n        return input_text\n\n    output_summary = \"\"\n    for segment in generate_segments(input_text):\n        try:\n            max_length = len(segment.split()) # Adjust max_length dynamically\n            summarized_segment = pipe(segment, max_length=max_length, min_length=1, do_sample=False)[0]['summary_text']\n            output_summary += summarized_segment\n            \n        except IndexError:\n            pass\n        except Exception as e:\n            print(f\"Error occurred: {e}\")\n    if len(output_summary.split()) >= 1020:\n            output_summary = summarization(output_summary)\n    count += 1\n    print(count, end=\" \")\n    return output_summary\ncount = 0","metadata":{"execution":{"iopub.status.busy":"2024-04-29T08:26:30.819758Z","iopub.execute_input":"2024-04-29T08:26:30.820073Z","iopub.status.idle":"2024-04-29T08:26:30.829256Z","shell.execute_reply.started":"2024-04-29T08:26:30.820048Z","shell.execute_reply":"2024-04-29T08:26:30.828488Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\nmodel_path = \"/kaggle/input/model-till-25000\"\n\n# Load the model\ntokenizer = GPT2Tokenizer.from_pretrained(model_path)\n# tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\nmodel.resize_token_embeddings(len(tokenizer))\n\n# If GPU is available, move the model to GPU\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T15:23:27.178943Z","iopub.execute_input":"2024-04-28T15:23:27.179283Z","iopub.status.idle":"2024-04-28T15:23:38.965496Z","shell.execute_reply.started":"2024-04-28T15:23:27.179250Z","shell.execute_reply":"2024-04-28T15:23:38.964537Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50258, 1024)\n    (wpe): Embedding(1024, 1024)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-23): 24 x GPT2Block(\n        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1024, out_features=50258, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"!pip install flask-ngrok >> /dev/null\n!pip install flask_cors\n!pip install pyngrok\nfrom flask import Flask,request,jsonify\nfrom pyngrok import ngrok\nfrom flask_cors import CORS\nfrom flask import Response\n!ngrok config add-authtoken 2cDjv295789xMxfOXzNy20lFY2C_2zrPAEF32U1teXtTXx2rr\nngrok.set_auth_token(\"2cDjv295789xMxfOXzNy20lFY2C_2zrPAEF32U1teXtTXx2rr\")","metadata":{"execution":{"iopub.status.busy":"2024-04-28T15:23:38.966876Z","iopub.execute_input":"2024-04-28T15:23:38.967201Z","iopub.status.idle":"2024-04-28T15:24:21.484806Z","shell.execute_reply.started":"2024-04-28T15:23:38.967175Z","shell.execute_reply":"2024-04-28T15:24:21.483827Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting flask_cors\n  Downloading Flask_Cors-4.0.0-py2.py3-none-any.whl.metadata (5.4 kB)\nRequirement already satisfied: Flask>=0.9 in /opt/conda/lib/python3.10/site-packages (from flask_cors) (3.0.3)\nRequirement already satisfied: Werkzeug>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from Flask>=0.9->flask_cors) (3.0.2)\nRequirement already satisfied: Jinja2>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from Flask>=0.9->flask_cors) (3.1.2)\nRequirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from Flask>=0.9->flask_cors) (2.2.0)\nRequirement already satisfied: click>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from Flask>=0.9->flask_cors) (8.1.7)\nRequirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from Flask>=0.9->flask_cors) (1.7.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2>=3.1.2->Flask>=0.9->flask_cors) (2.1.3)\nDownloading Flask_Cors-4.0.0-py2.py3-none-any.whl (14 kB)\nInstalling collected packages: flask_cors\nSuccessfully installed flask_cors-4.0.0\nCollecting pyngrok\n  Downloading pyngrok-7.1.6-py3-none-any.whl.metadata (7.4 kB)\nRequirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.10/site-packages (from pyngrok) (6.0.1)\nDownloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\nInstalling collected packages: pyngrok\nSuccessfully installed pyngrok-7.1.6\nAuthtoken saved to configuration file: /root/.config/ngrok/ngrok.yml                                \n","output_type":"stream"}]},{"cell_type":"code","source":"def generation(query):\n    headline = []\n    input_ids = tokenizer(query, return_tensors='pt')\n    # Move the input to GPU if available\n    input_ids = input_ids.to(\"cuda\")\n\n    # Generate 5 different outputs\n    num_outputs = 7\n    generated_outputs = []\n    max_new_tokens = 24\n    for _ in range(num_outputs):\n        # Generate output using the model\n        output = model.generate(\n            input_ids=input_ids[\"input_ids\"],\n            attention_mask=input_ids['attention_mask'],\n            length_penalty=0.8,\n            min_new_tokens=5,\n            max_new_tokens=max_new_tokens+_,\n            num_beams=8,\n            no_repeat_ngram_size=2,\n            early_stopping=True,\n            do_sample=True\n        )\n        generated_outputs.append(tokenizer.decode(output[0]))\n    for i, output_text in enumerate(generated_outputs):\n        # Remove <pad> tokens\n        output_text = output_text.replace(\"<pad>\", \"\")\n        output_text = output_text.replace(\"<|endoftext|>\", \"\")\n\n        # Find the index of \"TL;DR:\"\n        tldr_index = output_text.find(\"TL;DR:\")\n        if tldr_index != -1:\n            # Extract the text after \"TL;DR:\"\n            output_text = output_text[tldr_index + len(\"TL;DR:\"):]\n\n        # Remove commas from the output text\n        output_text = output_text.replace(\",\", \"\")\n        headline.append(output_text.strip())\n    return headline\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-28T15:24:21.486852Z","iopub.execute_input":"2024-04-28T15:24:21.488298Z","iopub.status.idle":"2024-04-28T15:24:21.497946Z","shell.execute_reply.started":"2024-04-28T15:24:21.488249Z","shell.execute_reply":"2024-04-28T15:24:21.497025Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"app = Flask(__name__)\nCORS(app)\n\n@app.before_request\ndef basic_authentication():\n    if request.method.lower() == 'options':\n        return Response()\n\n\n@app.route(\"/\")\ndef home():\n    return jsonify({'result': \"result\"})\n\n@app.route('/api/data/',methods=['POST'])\ndef get_data():\n    article = request.get_json()\n    sentence = summarization(article)\n    query = sentence + '\\nTL;DR:'\n    result = generation(query)\n    return result\n\nngrok_tunnel = ngrok.connect(5000)\nprint(' * Tunnel URL:', ngrok_tunnel.public_url)\n\nif __name__ == \"__main__\":\n    app.run()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T15:24:21.499058Z","iopub.execute_input":"2024-04-28T15:24:21.499389Z","iopub.status.idle":"2024-04-28T16:18:16.458124Z","shell.execute_reply.started":"2024-04-28T15:24:21.499365Z","shell.execute_reply":"2024-04-28T16:18:16.456987Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":" * Tunnel URL: https://ede4-34-171-144-64.ngrok-free.app\n * Serving Flask app '__main__'\n * Debug mode: off\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1 ","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2 ","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3 ","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/socketserver.py\", line 683, in process_request_thread\n    self.finish_request(request, client_address)\n  File \"/opt/conda/lib/python3.10/socketserver.py\", line 360, in finish_request\n    self.RequestHandlerClass(request, client_address, self)\n  File \"/opt/conda/lib/python3.10/socketserver.py\", line 747, in __init__\n    self.handle()\n  File \"/opt/conda/lib/python3.10/site-packages/werkzeug/serving.py\", line 390, in handle\n    super().handle()\n  File \"/opt/conda/lib/python3.10/http/server.py\", line 433, in handle\n    self.handle_one_request()\n  File \"/opt/conda/lib/python3.10/http/server.py\", line 421, in handle_one_request\n    method()\n  File \"/opt/conda/lib/python3.10/site-packages/werkzeug/serving.py\", line 362, in run_wsgi\n    execute(self.server.app)\n  File \"/opt/conda/lib/python3.10/site-packages/werkzeug/serving.py\", line 323, in execute\n    application_iter = app(environ, start_response)\n  File \"/opt/conda/lib/python3.10/site-packages/flask/app.py\", line 1498, in __call__\n    return self.wsgi_app(environ, start_response)\n  File \"/opt/conda/lib/python3.10/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/opt/conda/lib/python3.10/site-packages/flask/app.py\", line 880, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/opt/conda/lib/python3.10/site-packages/flask/app.py\", line 865, in dispatch_request\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n  File \"/tmp/ipykernel_34/1952841470.py\", line 17, in get_data\n    sentence = summarization(article)\n  File \"/tmp/ipykernel_34/4139021193.py\", line 21, in summarization\n    summarized_segment = pipe(segment, max_length=max_length, min_length=1, do_sample=False)[0]['summary_text']\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py\", line 269, in __call__\n    return super().__call__(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py\", line 167, in __call__\n    result = super().__call__(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"4 ","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"5 ","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"}]}]}