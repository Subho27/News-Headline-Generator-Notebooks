{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8183749,"sourceType":"datasetVersion","datasetId":4845559},{"sourceId":8260077,"sourceType":"datasetVersion","datasetId":4902460},{"sourceId":8261819,"sourceType":"datasetVersion","datasetId":4902485},{"sourceId":8262520,"sourceType":"datasetVersion","datasetId":4904310}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-29T17:04:42.466413Z","iopub.execute_input":"2024-04-29T17:04:42.466748Z","iopub.status.idle":"2024-04-29T17:04:43.311799Z","shell.execute_reply.started":"2024-04-29T17:04:42.466720Z","shell.execute_reply":"2024-04-29T17:04:43.310874Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/till-20000/till_20000.csv\n/kaggle/input/till-39000/till_39000.csv\n/kaggle/input/model-till-30000/config.json\n/kaggle/input/model-till-30000/merges.txt\n/kaggle/input/model-till-30000/vocab.json\n/kaggle/input/model-till-30000/tokenizer_config.json\n/kaggle/input/model-till-30000/model.safetensors\n/kaggle/input/model-till-30000/special_tokens_map.json\n/kaggle/input/model-till-30000/added_tokens.json\n/kaggle/input/model-till-30000/generation_config.json\n/kaggle/input/till-32000/till_32000.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Importing Dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/till-32000/till_32000.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T13:19:21.568341Z","iopub.execute_input":"2024-04-29T13:19:21.568734Z","iopub.status.idle":"2024-04-29T13:19:21.904145Z","shell.execute_reply.started":"2024-04-29T13:19:21.568709Z","shell.execute_reply":"2024-04-29T13:19:21.903077Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df1 = pd.read_csv(\"/kaggle/input/till-20000/till_20000.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T13:19:21.906576Z","iopub.execute_input":"2024-04-29T13:19:21.906909Z","iopub.status.idle":"2024-04-29T13:19:22.835532Z","shell.execute_reply.started":"2024-04-29T13:19:21.906881Z","shell.execute_reply":"2024-04-29T13:19:22.834472Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df.drop(\"Unnamed: 0\", axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T13:19:22.836830Z","iopub.execute_input":"2024-04-29T13:19:22.837176Z","iopub.status.idle":"2024-04-29T13:19:22.848031Z","shell.execute_reply.started":"2024-04-29T13:19:22.837147Z","shell.execute_reply":"2024-04-29T13:19:22.847150Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-29T13:19:22.849110Z","iopub.execute_input":"2024-04-29T13:19:22.849383Z","iopub.status.idle":"2024-04-29T13:19:22.868383Z","shell.execute_reply.started":"2024-04-29T13:19:22.849361Z","shell.execute_reply":"2024-04-29T13:19:22.867544Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                               title  \\\n0  Yes, Trump really is fighting with a lawyer fo...   \n1  In Passing for Human, Liana Finck Illustrates ...   \n2  Dick's Sporting Goods won't sell assault-style...   \n3  A Utopian Suburb Where Residents Underwrite an...   \n4  Watch a Massive Tarantula Drag an Opossum Arou...   \n\n                                             article  \\\n0  The president of the United States is in a Twi...   \n1  The New Yorker cartoonist calls her new graphi...   \n2  In the absence of any meaningful gun regulatio...   \n3  Serenbe, a bedroom community of Atlanta with f...   \n4   Tarantulas are often cast as creepy crawlers ...   \n\n                                             summary  \n0  The president of the United States is in a Twi...  \n1  New Yorker cartoonist Liana Finck's new graphi...  \n2  In the absence of any meaningful gun regulatio...  \n3  Brandon Hinman is the first paid staff member ...  \n4   Tarantulas are often cast as creepy crawlers ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>article</th>\n      <th>summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Yes, Trump really is fighting with a lawyer fo...</td>\n      <td>The president of the United States is in a Twi...</td>\n      <td>The president of the United States is in a Twi...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In Passing for Human, Liana Finck Illustrates ...</td>\n      <td>The New Yorker cartoonist calls her new graphi...</td>\n      <td>New Yorker cartoonist Liana Finck's new graphi...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dick's Sporting Goods won't sell assault-style...</td>\n      <td>In the absence of any meaningful gun regulatio...</td>\n      <td>In the absence of any meaningful gun regulatio...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A Utopian Suburb Where Residents Underwrite an...</td>\n      <td>Serenbe, a bedroom community of Atlanta with f...</td>\n      <td>Brandon Hinman is the first paid staff member ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Watch a Massive Tarantula Drag an Opossum Arou...</td>\n      <td>Tarantulas are often cast as creepy crawlers ...</td>\n      <td>Tarantulas are often cast as creepy crawlers ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['summary'] = df['summary'].str.replace(r'\\s{2,}', ' ', regex=True).str.strip()","metadata":{"execution":{"iopub.status.busy":"2024-04-29T13:19:22.869377Z","iopub.execute_input":"2024-04-29T13:19:22.869709Z","iopub.status.idle":"2024-04-29T13:19:23.015562Z","shell.execute_reply.started":"2024-04-29T13:19:22.869687Z","shell.execute_reply":"2024-04-29T13:19:23.014813Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Loading the Model","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\nmodel_path = \"/kaggle/input/model-till-30000\"\n\n# Load the model\ntokenizer = GPT2Tokenizer.from_pretrained(model_path)\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\n\n# If GPU is available, move the model to GPU\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T13:19:23.016651Z","iopub.execute_input":"2024-04-29T13:19:23.016970Z","iopub.status.idle":"2024-04-29T13:19:38.443743Z","shell.execute_reply.started":"2024-04-29T13:19:23.016942Z","shell.execute_reply":"2024-04-29T13:19:38.442799Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50258, 1024)\n    (wpe): Embedding(1024, 1024)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-23): 24 x GPT2Block(\n        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1024, out_features=50258, bias=False)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"### DataSet Preparation","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nfrom datasets import Dataset, load_dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-29T13:19:38.444979Z","iopub.execute_input":"2024-04-29T13:19:38.445418Z","iopub.status.idle":"2024-04-29T13:19:38.937298Z","shell.execute_reply.started":"2024-04-29T13:19:38.445392Z","shell.execute_reply":"2024-04-29T13:19:38.936547Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df[\"gpt2_summary\"] = df1[\"summary\"] + '\\nTL;DR:' + df[\"title\"]","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:11:01.956110Z","iopub.execute_input":"2024-04-29T15:11:01.956993Z","iopub.status.idle":"2024-04-29T15:11:01.993219Z","shell.execute_reply.started":"2024-04-29T15:11:01.956957Z","shell.execute_reply":"2024-04-29T15:11:01.992470Z"},"trusted":true},"execution_count":214,"outputs":[]},{"cell_type":"code","source":"df1[\"gpt2_summary\"] = df[\"summary\"] + '\\nTL;DR:' + df1[\"title\"]","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:11:05.941197Z","iopub.execute_input":"2024-04-29T15:11:05.941872Z","iopub.status.idle":"2024-04-29T15:11:05.959201Z","shell.execute_reply.started":"2024-04-29T15:11:05.941840Z","shell.execute_reply":"2024-04-29T15:11:05.958083Z"},"trusted":true},"execution_count":215,"outputs":[]},{"cell_type":"code","source":"new_df = df[['gpt2_summary']]\neval_df = df[['gpt2_summary']].iloc[1800:2000]","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:11:15.805270Z","iopub.execute_input":"2024-04-29T15:11:15.806165Z","iopub.status.idle":"2024-04-29T15:11:15.812274Z","shell.execute_reply.started":"2024-04-29T15:11:15.806136Z","shell.execute_reply":"2024-04-29T15:11:15.811327Z"},"trusted":true},"execution_count":216,"outputs":[]},{"cell_type":"code","source":"new_df = Dataset.from_pandas(new_df)\neval_df = Dataset.from_pandas(eval_df)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:11:18.773031Z","iopub.execute_input":"2024-04-29T15:11:18.773945Z","iopub.status.idle":"2024-04-29T15:11:18.826506Z","shell.execute_reply.started":"2024-04-29T15:11:18.773910Z","shell.execute_reply":"2024-04-29T15:11:18.825738Z"},"trusted":true},"execution_count":217,"outputs":[]},{"cell_type":"code","source":"new_df","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:11:20.238527Z","iopub.execute_input":"2024-04-29T15:11:20.239214Z","iopub.status.idle":"2024-04-29T15:11:20.244968Z","shell.execute_reply.started":"2024-04-29T15:11:20.239184Z","shell.execute_reply":"2024-04-29T15:11:20.244006Z"},"trusted":true},"execution_count":218,"outputs":[{"execution_count":218,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['gpt2_summary'],\n    num_rows: 2000\n})"},"metadata":{}}]},{"cell_type":"code","source":"eval_df","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:11:21.119159Z","iopub.execute_input":"2024-04-29T15:11:21.119819Z","iopub.status.idle":"2024-04-29T15:11:21.125353Z","shell.execute_reply.started":"2024-04-29T15:11:21.119790Z","shell.execute_reply":"2024-04-29T15:11:21.124524Z"},"trusted":true},"execution_count":219,"outputs":[{"execution_count":219,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['gpt2_summary'],\n    num_rows: 200\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"### Encoding the Inputs","metadata":{}},{"cell_type":"code","source":"def convert_examples_to_features(example_batch):\n    input_encodings = tokenizer(example_batch['gpt2_summary'] , max_length = 1024, padding = True, truncation = True )\n    \n    #with tokenizer.as_target_tokenizer():\n        #target_encodings = tokenizer(example_batch['title'], max_length = 32, padding = True, truncation = True )\n        \n    return {\n        'input_ids' : input_encodings['input_ids'],\n        'attention_mask': input_encodings['attention_mask'],\n        'labels': input_encodings['input_ids']\n    }\n    \nnew_df_pt = new_df.map(convert_examples_to_features, batched = True)\neval_df_pt = eval_df.map(convert_examples_to_features, batched = True)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:11:22.216241Z","iopub.execute_input":"2024-04-29T15:11:22.217040Z","iopub.status.idle":"2024-04-29T15:11:38.420820Z","shell.execute_reply.started":"2024-04-29T15:11:22.217008Z","shell.execute_reply":"2024-04-29T15:11:38.419863Z"},"trusted":true},"execution_count":220,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5840e6eed7ea4e5c8382c6d925f7c1a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"734bf24607104b56a29a530a4018776a"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\nseq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:11:38.422697Z","iopub.execute_input":"2024-04-29T15:11:38.422984Z","iopub.status.idle":"2024-04-29T15:11:38.427626Z","shell.execute_reply.started":"2024-04-29T15:11:38.422957Z","shell.execute_reply":"2024-04-29T15:11:38.426549Z"},"trusted":true},"execution_count":221,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\nfrom accelerate import Accelerator, DataLoaderConfiguration\n\n# Create DataLoaderConfiguration object\ndataloader_config = DataLoaderConfiguration(\n    dispatch_batches=None,  # Change None to your desired value\n    split_batches=False,\n    even_batches=True,\n    use_seedable_sampler=True\n)\n\n# Create Accelerator object with DataLoaderConfiguration\naccelerator = Accelerator(dataloader_config=dataloader_config)\n\n\n# Define training arguments with gradient accumulation, dropout, and validation\ntrainer_args = TrainingArguments(\n    output_dir='news_headline',\n    num_train_epochs=10,\n    warmup_steps=500,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    weight_decay=0.01,\n    logging_steps=15,\n    gradient_accumulation_steps=16,\n    learning_rate=3e-5,  # Specify the learning rate here\n    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n    overwrite_output_dir=True,\n    dataloader_num_workers=accelerator.num_processes\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:11:38.428715Z","iopub.execute_input":"2024-04-29T15:11:38.428963Z","iopub.status.idle":"2024-04-29T15:11:38.466336Z","shell.execute_reply.started":"2024-04-29T15:11:38.428941Z","shell.execute_reply":"2024-04-29T15:11:38.465482Z"},"trusted":true},"execution_count":222,"outputs":[]},{"cell_type":"code","source":"# Create Trainer object\ntrainer = Trainer(\n    model=model,\n    args=trainer_args,\n    tokenizer=tokenizer,\n    data_collator=seq2seq_data_collator,\n    train_dataset=new_df_pt,\n    eval_dataset=eval_df_pt\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:11:38.468522Z","iopub.execute_input":"2024-04-29T15:11:38.468846Z","iopub.status.idle":"2024-04-29T15:11:39.099224Z","shell.execute_reply.started":"2024-04-29T15:11:38.468816Z","shell.execute_reply":"2024-04-29T15:11:39.098374Z"},"trusted":true},"execution_count":223,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Computing Rouge Score","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\nfrom rouge import Rouge\nrouge = Rouge()\n\ndef calculate_rouge_scores(df, model, tokenizer):\n    rouge_scores = []\n\n    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating ROUGE scores\"):\n        # Define the reference summary\n        reference_summary = row[\"title\"]\n\n        output = pipe(row[\"article\"], **generation_args)\n        # Define the input text for GPT-2\n        input_text = output[0]['generated_text'] + '\\nTL;DR:'\n\n        # Tokenize input text\n        input_ids = tokenizer(input_text, max_length = 900, truncation=True, return_tensors=\"pt\").to(\"cuda\")\n        # Generate predictions\n        output = model.generate(input_ids=input_ids[\"input_ids\"],\n                                attention_mask=input_ids[\"attention_mask\"],\n                                length_penalty=0.8,\n                                min_new_tokens=7,\n                                max_new_tokens=24,\n                                num_beams=8,\n                                no_repeat_ngram_size=2,\n                                early_stopping=True)\n\n        # Decode the generated sequence\n        output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n        tldr_index = output_text.find(\"TL;DR:\")\n        if tldr_index != -1:\n            # Extract the text after \"TL;DR:\"\n            output_text = output_text[tldr_index + len(\"TL;DR:\"):]\n\n        # Remove commas from the output text\n        output_text = output_text.replace(\",\", \"\")\n\n        generated_text = output_text.strip()\n\n        # Calculate ROUGE for the generated and reference summaries\n        scores = rouge.get_scores(generated_text, reference_summary)\n        rouge_scores.append(scores)\n\n    return rouge_scores\n\n# Example usage:\nsampled_df = df1.sample(n=40, random_state=42)\nrouge_scores = calculate_rouge_scores(sampled_df, model, tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T13:20:37.466228Z","iopub.execute_input":"2024-04-29T13:20:37.466796Z","iopub.status.idle":"2024-04-29T13:54:13.472992Z","shell.execute_reply.started":"2024-04-29T13:20:37.466769Z","shell.execute_reply":"2024-04-29T13:54:13.472084Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Calculating ROUGE scores:   0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:   2%|▎         | 1/40 [01:03<40:57, 63.02s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:   5%|▌         | 2/40 [01:28<25:49, 40.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:   8%|▊         | 3/40 [02:26<30:10, 48.93s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  10%|█         | 4/40 [03:25<31:40, 52.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  12%|█▎        | 5/40 [04:27<32:44, 56.13s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  15%|█▌        | 6/40 [05:26<32:16, 56.95s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  18%|█▊        | 7/40 [05:27<21:23, 38.88s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  20%|██        | 8/40 [06:26<24:04, 45.13s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  22%|██▎       | 9/40 [07:25<25:39, 49.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  25%|██▌       | 10/40 [07:34<18:27, 36.93s/it]--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n    app.start()\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 701, in start\n    self.io_loop.start()\n  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n    await self.process_one()\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n    await dispatch(*args)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n    await result\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_34/3697539025.py\", line 49, in <module>\n    rouge_scores = calculate_rouge_scores(sampled_df, model, tokenizer)\n  File \"/tmp/ipykernel_34/3697539025.py\", line 13, in calculate_rouge_scores\n    output = pipe(row[\"article\"], **generation_args)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 240, in __call__\n    return super().__call__(text_inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  28%|██▊       | 11/40 [08:09<17:35, 36.41s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  30%|███       | 12/40 [08:27<14:21, 30.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  32%|███▎      | 13/40 [09:18<16:32, 36.75s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  35%|███▌      | 14/40 [10:17<18:52, 43.56s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  38%|███▊      | 15/40 [11:16<20:06, 48.27s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  40%|████      | 16/40 [11:20<13:56, 34.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  42%|████▎     | 17/40 [11:42<11:54, 31.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  45%|████▌     | 18/40 [12:42<14:34, 39.75s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  48%|████▊     | 19/40 [13:42<16:02, 45.85s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  50%|█████     | 20/40 [14:41<16:35, 49.80s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  52%|█████▎    | 21/40 [15:40<16:39, 52.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  55%|█████▌    | 22/40 [16:40<16:24, 54.72s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  57%|█████▊    | 23/40 [17:41<16:02, 56.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  60%|██████    | 24/40 [18:41<15:24, 57.77s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  62%|██████▎   | 25/40 [18:50<10:46, 43.13s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  65%|██████▌   | 26/40 [19:50<11:11, 47.97s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  68%|██████▊   | 27/40 [20:08<08:28, 39.09s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  70%|███████   | 28/40 [21:19<09:44, 48.71s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  72%|███████▎  | 29/40 [22:18<09:30, 51.85s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  75%|███████▌  | 30/40 [23:17<09:00, 54.00s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  78%|███████▊  | 31/40 [24:16<08:19, 55.54s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  80%|████████  | 32/40 [25:28<08:02, 60.36s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  82%|████████▎ | 33/40 [26:27<06:59, 59.97s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  85%|████████▌ | 34/40 [27:27<05:59, 59.89s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  88%|████████▊ | 35/40 [28:37<05:15, 63.05s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  90%|█████████ | 36/40 [29:36<04:07, 61.82s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  92%|█████████▎| 37/40 [30:35<03:03, 61.04s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  95%|█████████▌| 38/40 [31:37<02:02, 61.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores:  98%|█████████▊| 39/40 [32:36<01:00, 60.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nCalculating ROUGE scores: 100%|██████████| 40/40 [33:35<00:00, 50.40s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Compute average scores\nrouge1_avg = sum(score[0]['rouge-1']['f'] for score in new_scores) / len(new_scores)\nrouge2_avg = sum(score[0]['rouge-2']['f'] for score in new_scores) / len(new_scores)\nrougeL_avg = sum(score[0]['rouge-l']['f'] for score in new_scores) / len(new_scores)\n\n# Create a dictionary of average scores\navg_scores = {\n    'rouge1_avg': rouge1_avg,\n    'rouge2_avg': rouge2_avg,\n    'rougeL_avg': rougeL_avg\n}\n\n# Display average scores in a table\ndf_avg_scores = pd.DataFrame.from_dict(avg_scores, orient='index', columns=['Average Score'])\nprint(df_avg_scores)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:05:39.917036Z","iopub.execute_input":"2024-04-29T15:05:39.917391Z","iopub.status.idle":"2024-04-29T15:05:39.928582Z","shell.execute_reply.started":"2024-04-29T15:05:39.917363Z","shell.execute_reply":"2024-04-29T15:05:39.927023Z"},"trusted":true},"execution_count":213,"outputs":[{"name":"stdout","text":"            Average Score\nrouge1_avg       0.333333\nrouge2_avg       0.142857\nrougeL_avg       0.200000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Evaluating Rouge Score Current News Article","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom rouge import Rouge\n\n\n# Define the generated summary and the reference summary\nreference_summary = '''AI a 'fundamental change in the news ecosystem'''\n\n# Initialize the ROUGE object\nrouge = Rouge()\n\n# Define the input text for GPT-2\ninput_text = '''Artificial intelligence is shaking up journalism and in the short term will cause \"a fundamental change in the news ecosystem\", media expert David Caswell told AFP.\nA former employee at Yahoo! and BBC News Labs, the British broadcaster's innovation wing, Caswell spoke as industry leaders gathered in the Italian city of Perugia to discuss the biggest questions facing their trade.\n\"We don't know. But what we are trying to do is to understand all of the possibilities or as many of the possibilities as we can. But I think there are some things that are becoming clearer: one is the fact that more media will probably be created and originated and sourced by machines. So machines will do more gathering in a lot of journalism, will do more of the producing, the audio, the video and the text, and will create the kind of experiences of consumption that consumers have.\nThat is a very fundamental change in the information ecosystem in general, and the news ecosystem in particular. This is structurally different than the one that we're in now. We don't know how long it's going to take - it may be two, four, seven years. I think it's going to be faster because there is very little friction.\nPeople don't need news devices, new hardware, they don't need a lot of money as producers, they don't need technical expertise. All those things that were barriers in the previous generation of AI are no longer barriers, thanks to generative AI\".\n\"One class of development is in new tools that enables AI workflow, for example JP Politikens in Denmark focused on making their existing products and activities more efficient. But it is also a basis for transitioning their products, their workforce, the activities into this new AI world.\nThere is a tool that Google has built -- the code name is 'Genesis' -- that they are testing with publishers. Some publishers are building their own. There will be platform versions of these tools.\nThese are tools, you bring your news gathering on the left side: your PDF, transcripts, audios, videos.. roughly. It helps you do things like analysis, summaries, turn into scripts, audios. They're orchestrated by the tool.\nWhat the journalist is doing is coordinating the tool, verifying the content all the way through to the end, and editing. The job becomes using the tool, like an editorial manager of this AI tool.\nIt technically works. But that's a different thing than putting it in a newsroom in a large operation and use it day in day out, months in, months out. That's a big question: is it going to be enthusiastically adopted, to be used in a way that isn't very productive in the long run or will that enhance the productivity of newsroom dramatically?\"\n\"In the last decade it was very expensive. It was very difficult: You need the data, you had to build a data warehouse, have an enterprise deal with Amazon or Google cloud, you had to hire data scientists, to have a team of data engineers. it was a major investment. Only the BBC, the New York Times, this level of organisations could really afford it.\nThat's not true with generative AI. You can run news workflow through interfaces that you pay 20 dollars a month. You don't need to be a coder. All you need is motivation, enthusiasm and curiosity.\nThere's lots of people in news organisations that would not have been involved in AI in the past because they did not have the technical background and now they can just use it. It's a much more open form of AI: both smaller newsrooms can do a lot with, and more junior individuals in more established newsrooms can do a lot with. I think it's a good thing, but it's also a disruptive thing. Often the internal politics in newsrooms are disrupted by that\".\n\"AI has been around since the 1950s. But AI for practical purposes appeared with ChatGPT. It's going to be quite a while -- years -- before we really understand how to use them for valuable things. There are so many things that you can do with them.''' + '\\nTL;DR:'\n\n# Tokenize input text\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(\"cuda\")\n\n# Generate predictions\noutput = model.generate(input_ids=input_ids,\n                        length_penalty=0.9,\n                        min_new_tokens=7,\n                        max_new_tokens=24,\n                        num_beams=8,\n                        #no_repeat_ngram_size=2,\n                        early_stopping=True,\n                        do_sample=True)\n\n# Decode the generated sequence\noutput_text = tokenizer.decode(output[0], skip_special_tokens=True)\ntldr_index = output_text.find(\"TL;DR:\")\nif tldr_index != -1:\n    # Extract the text after \"TL;DR:\"\n    output_text = output_text[tldr_index + len(\"TL;DR:\"):]\n\n#Remove commas from the output text\noutput_text = output_text.replace(\",\", \"\")\n\ngenerated_text = output_text.strip()\n\n# Calculate ROUGE for the generated and reference summaries\nscores = rouge.get_scores(generated_text, reference_summary)\n# Print the results\nprint(scores)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T14:55:03.281085Z","iopub.execute_input":"2024-04-29T14:55:03.281464Z","iopub.status.idle":"2024-04-29T14:55:05.192374Z","shell.execute_reply.started":"2024-04-29T14:55:03.281435Z","shell.execute_reply":"2024-04-29T14:55:05.191413Z"},"trusted":true},"execution_count":193,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"[{'rouge-1': {'r': 0.5, 'p': 0.2222222222222222, 'f': 0.3076923034319527}, 'rouge-2': {'r': 0.42857142857142855, 'p': 0.16666666666666666, 'f': 0.23999999596800003}, 'rouge-l': {'r': 0.5, 'p': 0.2222222222222222, 'f': 0.3076923034319527}}]\n","output_type":"stream"}]},{"cell_type":"code","source":"generated_text","metadata":{"execution":{"iopub.status.busy":"2024-04-29T14:55:05.194040Z","iopub.execute_input":"2024-04-29T14:55:05.194324Z","iopub.status.idle":"2024-04-29T14:55:05.200146Z","shell.execute_reply.started":"2024-04-29T14:55:05.194300Z","shell.execute_reply":"2024-04-29T14:55:05.199239Z"},"trusted":true},"execution_count":194,"outputs":[{"execution_count":194,"output_type":"execute_result","data":{"text/plain":"'Artificial intelligence is shaking up journalism and in the short term will cause \"a fundamental change in the news ecosystem\"'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Saving Model","metadata":{}},{"cell_type":"code","source":"from IPython.display import FileLink\n\nfile_path = \"news_headline/checkpoint-1000/model.safetensors\"  # Specify the full path to the file\nlink = FileLink(file_path)\nlink\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T10:51:03.712232Z","iopub.execute_input":"2024-04-29T10:51:03.712989Z","iopub.status.idle":"2024-04-29T10:51:03.721535Z","shell.execute_reply.started":"2024-04-29T10:51:03.712957Z","shell.execute_reply":"2024-04-29T10:51:03.720382Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/news_headline/checkpoint-1000/model.safetensors","text/html":"<a href='news_headline/checkpoint-1000/model.safetensors' target='_blank'>news_headline/checkpoint-1000/model.safetensors</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}